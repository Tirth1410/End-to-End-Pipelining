
# üöÄ **Azure Machine Learning: Step-by-Step Pipelining**  

Welcome to the **Azure Machine Learning: Step-by-Step Pipelining** repository! üéâ This repository is your one-stop guide to learning how to build and deploy machine learning pipelines using Azure ML. The codes are 100% reproducible and it is beginner-friendly.

<p align="center">
  <img src="https://github.com/user-attachments/assets/f277cfa9-c2d9-4a6d-9766-e60e42b4a102" alt="Azure ML Pipeline Illustration" width="700" />
</p>


---

## üìÇ **Repository Structure**  

Here's a quick look at the repository contents:  

```plaintext
üì¶ Repository
‚îú‚îÄ‚îÄ üìÇ model
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ new_model_state_dict.pth
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vocab.txt
‚îú‚îÄ‚îÄ üìÇ scripts
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ score.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ script1.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ script2.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ script3.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ script4.py
‚îú‚îÄ‚îÄ üìÇ workspace
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dataset.csv
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ env.yml
‚îú‚îÄ‚îÄ üñºÔ∏è Pipeline_Structure.png
‚îú‚îÄ‚îÄ üìÑ README.md
‚îî‚îÄ‚îÄ üìÑ readme_MLOps.ipynb

```

## üìÇ Folder Overview

### üìÇ `model`
- Contains pre-trained models and vocabulary files.
  - üìÑ `new_model_state_dict.pth`: The saved state dictionary of the model.
  - üìÑ `vocab.txt`: Vocabulary used by the model.

### üìÇ `scripts`
- Includes Python scripts for various stages of the pipeline.
  - üìÑ `score.py`: Script to evaluate the model's performance.
  - üìÑ `script1.py`: First stage of the CI pipeline (e.g., data preprocessing).
  - üìÑ `script2.py`: Second stage of the CI pipeline (e.g., model training).
  - üìÑ `script3.py`: Third stage of CI pipeline (e.g., old and new model (the one after training) comparison).
  - üìÑ `script4.py`: The first and only stage of the CD Pipeline that is triggered only if new model is better than the old one.

### üìÇ `workspace`
- Workspace-related files like datasets and environment configurations.
  - üìÑ `dataset.csv`: Dataset used for training and evaluation.
  - üìÑ `env.yml`: Conda environment file to set up dependencies.

### üñºÔ∏è `Pipeline_Structure.png`
- Diagram illustrating the pipeline's structure or workflow.

### üìÑ `README.md`
- Main documentation file providing an overview of the repository.

### üìÑ `readme_MLOps.ipynb`
- A Jupyter Notebook used to create this project and 100% reproducible.

---

## üåü **Prerequisites**  

Before you start, make sure you have the following installed and understand the basics:  

### üì¶ Required Libraries:  
- **PyTorch**: For building and training neural networks.  
- **NumPy**: For numerical operations and data manipulation.  

### üìö Concepts to Know:  
- **Training and Testing Models**: Fit models to training data and evaluate them on test data.  
- **API Endpoints**: Deploy models as RESTful APIs for real-time inference.  

---

## ü§î **Why Use Pipelines in AI/ML?**  

Pipelines simplify and automate the machine learning lifecycle, addressing challenges like:  
- **Seamless Model Updates**: Automate retraining and deployment without manual intervention.  
- **Access Control**: Restrict changes to specific workflow stages.  
- **Scalability**: Handle growing datasets and complex models.  

> **Example**: Need to deploy an updated model? Pipelines ensure the transition is smooth, avoiding downtime or conflicts.

---

## üñºÔ∏è **Visual Workflow**  

<p align="center">
  <img src="https://github.com/Hyperspectral01/AzureML_Step-by-Step_Pipelining/blob/main/Pipeline_Structure.png" alt="Pipeline Steps" width="700" />
</p>  

---

## üõ†Ô∏è **Steps Involved in the Project**  

### 1Ô∏è‚É£ **Setup Workspace and Compute Resources**  
- Set up an **Azure workspace** and provision compute resources (e.g., Azure Virtual Machines) for data processing and model training.  
- These resources enable efficient handling of large-scale operations.  

### 2Ô∏è‚É£ **Upload Files and Resources**  
- Upload the pretrained model and `vocab.txt` to the Azure workspace.  
- Ensure these resources are available for future training and tokenization tasks.  

### 3Ô∏è‚É£ **Connect to Workspace**  
- Establish a secure connection to the Azure workspace using the **Azure ML SDK**.  
- This allows seamless access to Azure resources for model management and machine learning tasks.  

### 4Ô∏è‚É£ **Model Registration**  
- Register the pretrained model within Azure ML to facilitate:  
  - Versioning  
  - Easy access  
  - Future training or deployment steps  

### 5Ô∏è‚É£ **Upload Vocabulary File**  
- Upload the `vocab.txt` file to the workspace.  
- This file contains the vocabulary needed for tokenizing input data and training the model.  

### 6Ô∏è‚É£ **Dataset Preparation**  
- Store `dataset.csv` in **Azure Blob Storage** for easy access during preprocessing and model training.  
- Blob Storage ensures efficient handling of large-scale data.  

### 7Ô∏è‚É£ **script1.py: Data Preparation**  
- Develop `script1.py` to preprocess the dataset:  
  - Tokenize input data  
  - Extract features  
  - Split into training (`train_x`, `train_y`) and testing (`test_x`, `test_y`) sets  
- Save the preprocessed data for use in subsequent pipeline steps.  

### 8Ô∏è‚É£ **script2.py: Model Training**  
- Create `script2.py` to fine-tune the pretrained model using the training data (`train_x`, `train_y`).  
- Prepare the trained model for evaluation with test data (`test_x`, `test_y`).  

### 9Ô∏è‚É£ **script3.py: Model Comparison and Evaluation**  
- Implement `script3.py` to evaluate both old and new models using the test data (`test_x`, `test_y`).  
- Record performance in an output file:  
  - `True`: New model performs better  
  - Otherwise, `False`  

### üîü **Pipeline for Model Training and Evaluation**  
- Design a pipeline to automate:  
  - Model training  
  - Evaluation  
  - Comparison  
- Ensure reproducibility and integrate it into a **CI/CD workflow** for efficient updates.  

### 1Ô∏è‚É£1Ô∏è‚É£ **score.py: Web Input Processing and Prediction**  
- Develop `score.py` to handle incoming data from web applications:  
  - Tokenize text using `vocab.txt`  
  - Feed it to the trained model  
  - Generate predictions  
- Designed for real-time inference in production environments.  

### 1Ô∏è‚É£2Ô∏è‚É£ **script4.py: Deployment Script**  
- Write `script4.py` to automate the deployment of the latest model to an **Azure endpoint**.  
- Ensure the model is ready to serve predictions via web requests.  

### 1Ô∏è‚É£3Ô∏è‚É£ **Pipeline for Deployment**  
- Build a deployment pipeline that automates:  
  - Model registration  
  - Deployment scripts  
  - Scaling capabilities  
- Ensure efficient and seamless deployment.  

### 1Ô∏è‚É£4Ô∏è‚É£ **Function: MLOps**  
- Develop the `MLOps` function to orchestrate:  
  - Training pipeline  
  - Deployment pipeline  
- By passing the dataset name as an argument, this function ensures smooth execution of the entire ML lifecycle.  

---

## üìä **Results : Using the Endpoint** 

<p align="center">
  <img src="https://github.com/user-attachments/assets/8e67f6a7-8aea-42a1-88a4-103f38932578" alt="Results" width="700" />
</p>

## üéØ **Key Benefits**  

‚úÖ **Automation**: Reduces errors and ensures reproducibility.  
‚úÖ **Versioning**: Track datasets, models, and workflows.  
‚úÖ **Scalability**: Handles large-scale operations efficiently.  
‚úÖ **Collaboration**: Supports team-based workflows with access control.  

---

## üîó **Extra Links**  

- [What is MLOPS? (lang:Hindi)](https://www.youtube.com/watch?v=6SRifO6dmuE&t=664s&pp=ygUNd2hhdCBpcyBtbG9wcw%3D%3D)
- [Brief Idea about Devops in Azure (lang:en)](https://www.youtube.com/watch?v=4BibQ69MD8c&t=71s)  
- [Azure Machine Learning Documentation](https://learn.microsoft.com/en-us/azure/machine-learning/)  
- [PyTorch Official Website](https://pytorch.org/)  
- [NumPy Official Documentation](https://numpy.org/doc/)  

---

üéâ Thank you for exploring this project! Let's make ML workflows efficient and scalable with Azure Pipelines.  

